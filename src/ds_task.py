# -*- coding: utf-8 -*-
"""DS_task.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18g3pGW6G00zp17VUDIya7MDjUaKxZuVa
"""

import pandas as pd
import numpy as np
from datetime import datetime

# ML imports
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import FunctionTransformer, StandardScaler, LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.ensemble import RandomForestClassifier
from sklearn.base import clone

channel_master = pd.read_csv("/content/Channel_Master.csv")
video_summary = pd.read_csv("/content/Video_Summary.csv")

channel_master.head(1)

video_summary.head(1)

video_summary = video_summary.drop(columns=['Unnamed: 9', 'Unnamed: 10'])

video_summary.head(1)

# merged_df = channel_master.merge(
#     video_summary,
#     how="left",
#     left_on="channel_id",
#     right_on="Channelid"
# )
merged_df = pd.merge(channel_master, video_summary,left_on='Channel_Id', right_on='ChannelId')

merged_df.head(1)

duplicate_rows = merged_df[merged_df.duplicated()]
print(duplicate_rows)

selected_cols = [
    "Video_Id", "Title_x", "Uploader_x", "Channel_Id",
    "Duration_Sec", "View_Count", "Language",
    "SubscriberCount", "ShortDescription", "TotalViews"
]

clean_df = merged_df[selected_cols].rename(columns={
    "Title_x": "video_title",
    "Uploader_x": "video_uploader"
})

clean_df.head(2)

print(clean_df.isnull().sum())

clean_df["ShortDescription"] = clean_df["ShortDescription"].fillna("")

print(clean_df.isnull().sum())

clean_df["Video_Id"].duplicated().sum()

print(clean_df.isnull().values.any())

clean_df.dtypes

clean_df["text_all"] = (
    clean_df["video_title"].astype(str) + " " +
    clean_df["ShortDescription"].astype(str)
)

category_features = clean_df[["text_all"]]

clean_df.head(1)

adult_kw = ["18+", "sex", "nude", "xxx", "porn", "violence", "bloody", "murder"]
kids_kw = ["nursery", "cartoon", "kids", "baby", "rhymes", "disney", "lego"]

def label_audience(row):
    text = row["text_all"].lower()
    if any(k in text for k in adult_kw):
        return "18+"
    elif any(k in text for k in kids_kw):
        return "kids only"
    else:
        return "kids safe"

clean_df["Audience_Label"] = clean_df.apply(label_audience, axis=1)

clean_df.tail(5)

edu_kw = ["tutorial", "lesson", "learn", "course", "class", "education", "how to"]
ent_kw = ["movie", "music", "funny", "dance", "vlog", "comedy", "entertainment"]
tech_kw = ["tech", "gadget", "review", "unboxing", "software", "ai", "data", "python"]

def label_category(row):
    text = row["text_all"].lower()
    if any(k in text for k in edu_kw):
        return "Education"
    elif any(k in text for k in ent_kw):
        return "Entertainment"
    elif any(k in text for k in tech_kw):
        return "Tech"
    else:
        return "Other"

clean_df["Category_Label"] = clean_df.apply(label_category, axis=1)

clean_df.tail(5)

clean_df["Language"] = LabelEncoder().fit_transform(clean_df["Language"])

from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline as ImbPipeline
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

X = clean_df[["text_all", "Duration_Sec", "View_Count", "SubscriberCount", "TotalViews", "Language"]]
y = clean_df["Audience_Label"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

preprocessor = ColumnTransformer(
    transformers=[
        ("text", TfidfVectorizer(max_features=5000, stop_words="english"), "text_all"),
        ("num", StandardScaler(), ["Duration_Sec", "View_Count", "SubscriberCount", "TotalViews", "Language"])
    ]
)

aud_model = ImbPipeline([
    ("preprocess", preprocessor),
    ("smote", SMOTE(random_state=42)),
    ("clf", RandomForestClassifier(random_state=42, class_weight="balanced"))
])

aud_model.fit(X_train, y_train)

y_pred = aud_model.predict(X_test)
print("===== Audience Safety Model Report =====")
print(classification_report(y_test, y_pred))

clean_df.head(2)

clean_df["Audience_Pred"] = aud_model.predict(X)

clean_df.head()

X_cat = clean_df[["text_all", "Duration_Sec", "View_Count", "SubscriberCount", "TotalViews", "Language"]]
y_cat = clean_df["Category_Label"]

X_train_cat, X_test_cat, y_train_cat, y_test_cat = train_test_split(
    X_cat, y_cat, test_size=0.2, random_state=42, stratify=y_cat
)

preprocessor_cat = ColumnTransformer(
    transformers=[
        ("text", TfidfVectorizer(max_features=5000, stop_words="english"), "text_all"),
        ("num", StandardScaler(), ["Duration_Sec", "View_Count", "SubscriberCount", "TotalViews", "Language"])
    ]
)

cat_model = ImbPipeline([
    ("preprocess", preprocessor_cat),
    ("smote", SMOTE(random_state=42)),
    ("clf", RandomForestClassifier(random_state=42, class_weight="balanced"))
])

cat_model.fit(X_train_cat, y_train_cat)
y_pred_cat = cat_model.predict(X_test_cat)

print("===== Content Category Model Report =====")
print(classification_report(y_test_cat, y_pred_cat))

clean_df["Category_Pred"] = cat_model.predict(X_cat)

clean_df.tail()

export_df = clean_df[[
    "Video_Id",          # Unique ID
    "video_uploader",    # Channel Name
    "video_title",       # Video Title
    "Duration_Sec",      # Duration
    "View_Count",        # Views
    "SubscriberCount",   # Subscribers
    "TotalViews",        # (Proxy for Revenue if actual revenue not available)
    "Language",          # Language
    "Audience_Pred",     # Predicted Audience Safety
    "Category_Pred"      # Predicted Category
]]

export_df.to_csv("youtube_dashboard_data.csv", index=False)
print("✅ Export complete: youtube_dashboard_data.csv")

export_df.head()

export_df['Audience_Pred'].unique()

# Second model for Audience Safety

from sklearn.linear_model import LogisticRegression

# Create a second pipeline with Logistic Regression
aud_model_lr = Pipeline([
    ("preprocess", preprocessor),
    ("clf", LogisticRegression(random_state=42, class_weight="balanced", max_iter=1000))
])

# Fit and evaluate the new model
aud_model_lr.fit(X_train, y_train)
y_pred_lr = aud_model_lr.predict(X_test)
print("===== Second Audience Safety Model (Logistic Regression) Report =====")
print(classification_report(y_test, y_pred_lr))

# You can optionally add these predictions to your dataframe
clean_df["Audience_Pred_LR"] = aud_model_lr.predict(X)

# New preprocessor for the Multinomial Naive Bayes model
from sklearn.preprocessing import MinMaxScaler

preprocessor_mnb = ColumnTransformer(
    transformers=[
        ("text", TfidfVectorizer(max_features=5000, stop_words="english"), "text_all"),
        # Using MinMaxScaler for numerical features to avoid negative values
        ("num", MinMaxScaler(), ["Duration_Sec", "View_Count", "SubscriberCount", "TotalViews", "Language"])
    ],

    remainder='passthrough'
)

# Second model for Content Category
from sklearn.naive_bayes import MultinomialNB

# Creating a second pipeline with Multinomial Naive Bayes
cat_model_mnb = Pipeline([
    ("preprocess", preprocessor_mnb),
    ("clf", MultinomialNB())
])

# Fitting and evaluating the new model
cat_model_mnb.fit(X_train_cat, y_train_cat)
y_pred_mnb = cat_model_mnb.predict(X_test_cat)
print("===== Second Content Category Model (Multinomial Naive Bayes) Report =====")
print(classification_report(y_test_cat, y_pred_mnb))

import pickle


aud_model_path = "audience_safety_model.pkl"
cat_model_path = "content_category_model.pkl"


try:
    with open(aud_model_path, 'wb') as file:
        pickle.dump(aud_model, file)
    print(f"✅ Audience Safety Model saved to {aud_model_path}")
except Exception as e:
    print(f"Error saving Audience Safety Model: {e}")


try:
    with open(cat_model_path, 'wb') as file:
        pickle.dump(cat_model, file)
    print(f"✅ Content Category Model saved to {cat_model_path}")
except Exception as e:
    print(f"Error saving Content Category Model: {e}")

